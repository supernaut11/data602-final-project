{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import pandas\n",
    "\n",
    "pandas.set_option('display.max_columns', None)\n",
    "pandas.set_option('display.max_rows', 25)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Setting up the environment\n",
    "\n",
    "The next couple cells define global variables used throughout the notebook and ingest the training data that will be used to build a data model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set up some variables for the training data paths\n",
    "BASE_DATA_PATH = os.path.join('..', 'data')\n",
    "TRAIN_DATA_PATH = os.path.join(BASE_DATA_PATH, 'train')\n",
    "\n",
    "TRAINING_PROVIDERS_PATH = os.path.join(TRAIN_DATA_PATH, 'Train-1542865627584.csv')\n",
    "TRAINING_INPATIENT_PATH = os.path.join(TRAIN_DATA_PATH, \n",
    "    'Train_Inpatientdata-1542865627584.csv')\n",
    "TRAINING_OUTPATIENT_PATH = os.path.join(TRAIN_DATA_PATH, \n",
    "    'Train_Outpatientdata-1542865627584.csv')\n",
    "TRAINING_BENEFICIARY_PATH = os.path.join(TRAIN_DATA_PATH, \n",
    "    'Train_Beneficiarydata-1542865627584.csv')\n",
    "\n",
    "# Defines the default behavior for responding to values that do not exist in data set\n",
    "DEFAULT_NA_VALUE = \"None\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all of the training data\n",
    "train_providers = pandas.read_csv(TRAINING_PROVIDERS_PATH)\n",
    "train_inpatient = pandas.read_csv(TRAINING_INPATIENT_PATH)\n",
    "train_outpatient = pandas.read_csv(TRAINING_OUTPATIENT_PATH)\n",
    "train_beneficiary = pandas.read_csv(TRAINING_BENEFICIARY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How similar are the inpatient/outpatient data? Let's see how many columns are\n",
    "# unique between the two data sets\n",
    "inp_unique_cols = [x for x in train_inpatient.columns if x not in train_outpatient.columns]\n",
    "print(f'Unique columns in inpatient: {inp_unique_cols}')\n",
    "\n",
    "outp_unique_cols = [x for x in train_outpatient.columns if x not in train_inpatient.columns]\n",
    "print(f'Unique columns in outpatient: {outp_unique_cols}')"
   ]
  },
  {
   "source": [
    "# Merging claim data sets\n",
    "\n",
    "Based on what we know about the columns in the inpatient/outpatient tables, we can safely merge those datasets with minimal disruption. Recommend combining all of the claims into a single dataframe that has all of the data from each table with the following columns added:\n",
    "- Additional column for inpatient vs outpatient flag\n",
    "- Columns unique to inpatient (i.e., AdmissionDt, DischargeDt, DiagnosisGroupCode)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a flag to each of the data sets to indicate whether they came from the\n",
    "# inpatient vs. outpatient frames\n",
    "train_inpatient['In/Out'] = 'In'\n",
    "train_outpatient['In/Out'] = 'Out'\n",
    "\n",
    "# Use the concat operation to create a union of the two data sets. This \n",
    "# automatically handles any missing columns between the two sets, so no need\n",
    "# to manually add missing columns to the outpatient data first.\n",
    "train_claims = pandas.concat([train_inpatient, train_outpatient], ignore_index=True)\n",
    "\n",
    "train_claims"
   ]
  },
  {
   "source": [
    "# Merging patients with claims\n",
    "\n",
    "Using the beneficiary column in the claim data, we can also populate details about the patient in each of the transactions.\n",
    "\n",
    "This potentially allows us to engineer some additional features that are specific to beneficiaries across transactions. For example, how long has it been since a beneficiary's last claim?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a left outer join in case there are any claims that don't have any corresponding beneficiaries\n",
    "# defined in the beneficiary data set\n",
    "train_claims = train_claims.merge(train_beneficiary, how='left', on='BeneID')\n",
    "\n",
    "# TODO: Are there any claims that don't have corresponding beneficiary definitions?\n",
    "\n",
    "train_claims"
   ]
  },
  {
   "source": [
    "# Cleaning data\n",
    "\n",
    "There are some fields that have empty data. We need a way to properly deal with those fields so that analysis can be performed.\n",
    "\n",
    "It's probably too early to throw data out, so instead give these fields a friendly value like \"None\" and we can revisit whether we can throw them out entirely later.\n",
    "\n",
    "## Cleanup in the next cell:\n",
    "- In/Out column - Changed to boolean value, where 0 -> outpatient, 1-> inpatient\n",
    "- (*)Physician columns - Remove PHY prefix from provider IDs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use apply() to map columns to new values that have been sanitized (i.e., transform from strings into numeric)\n",
    "train_claims['In/Out'] = train_claims['In/Out'].apply(lambda x: 1 if x == 'In' else 0)\n",
    "\n",
    "# The next few transforms require checks using pandas.isnull() to prevent errors for empty cells\n",
    "train_claims['AttendingPhysician'] = train_claims['AttendingPhysician'].apply(lambda ap: ap.split('PHY')[-1] if not pandas.isnull(ap) else ap)\n",
    "train_claims['OperatingPhysician'] = train_claims['OperatingPhysician'].apply(lambda op: op.split('PHY')[-1] if not pandas.isnull(op) else op)\n",
    "train_claims['OtherPhysician'] = train_claims['OtherPhysician'].apply(lambda op: op.split('PHY')[-1] if not pandas.isnull(op) else op)\n",
    "\n",
    "train_claims"
   ]
  },
  {
   "source": [
    "## Cleanup in the next cell:\n",
    "- Some of the diagnosis code values are prefixed with a character. Remove them to make values numeric.\n",
    "\n",
    "> **TODO: Decide whether we need to get all fancy pants and save off the various character flags\n",
    "          that appear alongside the charge codes. Looks like 'E' and 'V' are the ones that show\n",
    "          up most, but maybe there are others?**\n",
    "\n",
    "> **TODO: Some resources online recommend using one-hot encoding to represent categorical data for decision tree algorithms:\n",
    "https://stackoverflow.com/questions/38108832/passing-categorical-data-to-sklearn-decision-tree\n",
    "Answer suggests one-hot is not super performant, but that might be acceptable since this data set isn't enormous...**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of diagnosis code columns, because hard-coding makes me sad\n",
    "diagnosis_code_cols = [f'ClmDiagnosisCode_{x+1}' for x in range(10)]\n",
    "\n",
    "# Strip ASCII characters from all of the claim diagnosis codes.\n",
    "import string\n",
    "for col in diagnosis_code_cols:\n",
    "    train_claims[col] = train_claims[col].apply(lambda c: c.strip(string.ascii_letters) if not pandas.isnull(c) else c)\n",
    "\n",
    "# Sanity check that all of the claim codes are now integers\n",
    "assert sum([len([x for x in train_claims[col] if not pandas.isnull(x) and not str(x).isdigit()]) for col in diagnosis_code_cols]) == 0\n",
    "\n",
    "train_claims\n"
   ]
  },
  {
   "source": [
    "# Cleanup in the next cell:\n",
    "\n",
    "Some of the deductible information has NaN. For now, set these values to 0.\n",
    "\n",
    "> TODO: Is setting to 0 the right approach?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_claims['DeductibleAmtPaid'] = train_claims['DeductibleAmtPaid'].apply(lambda amt: 0 if pandas.isnull(amt) else amt)\n",
    "\n",
    "train_claims"
   ]
  },
  {
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Features we can derive in the inpatient and outpatient datasets\n",
    "- Claim duration\n",
    "- Age of patient\n",
    "- Is Dead flag\n",
    "- Number of diagnoses\n",
    "- Number of procedures\n",
    "- Admission duration\n",
    "- Has attending phys\n",
    "- Has operating phys"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Calculate the age of the patient at the time their claim was initiated\n",
    "train_claims['PatientAge'] = (pandas.to_datetime(train_claims['ClaimStartDt'], format='%Y-%m-%d') - pandas.to_datetime(train_claims['DOB'], format='%Y-%m-%d'))/timedelta(days=365)\n",
    "\n",
    "# Calculate the duration of the patient's stay\n",
    "train_claims['PatientStayDur'] = (pandas.to_datetime(train_claims['DischargeDt'], format='%Y-%m-%d') - pandas.to_datetime(train_claims['AdmissionDt'], format='%Y-%m-%d'))/timedelta(days=1)\n",
    "train_claims['PatientStayDur'] = train_claims['PatientStayDur'].apply(lambda amt: 0 if pandas.isnull(amt) else amt)\n",
    "\n",
    "# Calculate the duration of the claim\n",
    "train_claims['ClaimDur'] = (pandas.to_datetime(train_claims['ClaimEndDt'], format='%Y-%m-%d') - pandas.to_datetime(train_claims['ClaimStartDt'], format='%Y-%m-%d') + timedelta(days=1))/timedelta(days=1)\n",
    "train_claims['ClaimDur'] = train_claims['ClaimDur'].apply(lambda amt: 0 if pandas.isnull(amt) else amt)\n",
    "\n",
    "train_claims['PatientDead'] = train_claims['DOD'].apply(lambda dod: 0 if pandas.isnull(dod) else 1)\n"
   ]
  },
  {
   "source": [
    "# Merging claim data with potential fraud indicators\n",
    "\n",
    "Once we have all of our features in place, we should merge with the potential fraud data set.\n",
    "This will let us set up independent/dependent variables for the decision tree to examine."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a left outer join so we don't throw away providers that might not have claims.\n",
    "train_providers = train_providers.merge(train_claims, how='left', on='Provider')\n",
    "\n",
    "# TODO: Sanity check that all providers have associated claim data.\n",
    "\n",
    "# Strip the PRV prefix from Provider IDs\n",
    "train_providers['Provider'] = train_providers['Provider'].apply(lambda ap: ap.split('PRV')[-1] if not pandas.isnull(ap) else ap)\n",
    "\n",
    "train_providers"
   ]
  },
  {
   "source": [
    "# Define independent variables\n",
    "\n",
    "Based on prior statistical evaluation and experimentation, we select independent variables to include in the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reintroduce age at claim when feature engineering section is complete\n",
    "features = ['InscClaimAmtReimbursed', 'DeductibleAmtPaid', 'PatientAge', 'PatientStayDur', 'ClaimDur', 'IPAnnualDeductibleAmt', 'IPAnnualReimbursementAmt', 'OPAnnualDeductibleAmt', 'OPAnnualReimbursementAmt', 'PatientDead']\n",
    "X = train_providers.reindex(columns=features)\n",
    "X"
   ]
  },
  {
   "source": [
    "# Define dependent variables\n",
    "\n",
    "Define the dependent variables that we are trying to predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_providers['PotentialFraud']\n",
    "Y"
   ]
  },
  {
   "source": [
    "# Compute decision tree\n",
    "\n",
    "Use the sklearn implementation of the decision tree classifier algorithm to build a decision tree from training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import decision tree\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "##Cant have any chracters in these fields, must be numbers\n",
    "clf = clf.fit(X, Y)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 20\n",
    "fig_size[1] = 20\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "import numpy as np\n",
    "sorted = Y.unique()\n",
    "sorted = np.sort(sorted)\n",
    "sorted = list(map(str, sorted))\n",
    "\n",
    "sorted\n",
    "\n",
    "print(clf.get_n_leaves())\n",
    "\n",
    "plot = tree.plot_tree(clf, rounded=True, filled=True, class_names=sorted, feature_names=features, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}